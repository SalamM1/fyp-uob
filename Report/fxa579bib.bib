
@article{wierzbicka_defining_1992,
	title = {Defining emotion concepts},
	volume = {16},
	issn = {0364-0213},
	url = {http://www.sciencedirect.com/science/article/pii/036402139290031O},
	doi = {10.1016/0364-0213(92)90031-O},
	abstract = {This article demonstrates that emotion concepts—including the so-called basic ones, such as anger or sadness—can be defined in terms of universal semantic primitives such as ‘good’, ‘bad’, ‘do’, ‘happen’, ‘know’, and ‘want’, in terms of which all areas of meaning, in all languages, can be rigorously and revealingly portrayed. The definitions proposed here take the form of certain prototypical scripts or scenarios, formulated in terms of thoughts, wants, and feelings. These scripts, however, can be seen as formulas providing rigorous specifications of necessary and sufficient conditions (not for emotions as such, but for emotion concepts), and they do not support the idea that boundaries between emotion concepts are “fuzzy.” On the contrary, the small set of universal semantic primitives employed here (which has emerged from two decades of empirical investigations by the author and colleagues) demonstrates that even apparent synonyms such as sad and unhappy embody different—and fully specifiable—conceptual structures.},
	pages = {539--581},
	number = {4},
	journaltitle = {Cognitive Science},
	shortjournal = {Cognitive Science},
	author = {Wierzbicka, Anna},
	urldate = {2020-04-09},
	date = {1992-10-01},
	langid = {english},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\salam\\Zotero\\storage\\SY8C96IB\\Wierzbicka - 1992 - Defining emotion concepts.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\salam\\Zotero\\storage\\N2VPURL2\\036402139290031O.html:text/html}
}

@article{cabanac_what_2002,
	title = {What is emotion?},
	volume = {60},
	issn = {0376-6357},
	url = {http://www.sciencedirect.com/science/article/pii/S0376635702000785},
	doi = {10.1016/S0376-6357(02)00078-5},
	abstract = {There is no consensus in the literature on a definition of emotion. The term is taken for granted in itself and, most often, emotion is defined with reference to a list: anger, disgust, fear, joy, sadness, and surprise. This article expands on a thesis that motivational states can be compared to each other by means of a common currency (Philos. Trans. Roy. Soc. Lond. 270 (1975) 265–293). I have previously argued that this common currency is pleasure. Such a conclusion is based not on introspective intuition, as with early pre-scientific psychology (Mill (1869)), but on experimental methods. As a follow-up to a definition of consciousness (Neurosci. Biobehav. Rev. 20 (1996) 33–40) as a four-dimensional experience (quality, intensity, hedonicity, and duration), I propose here that emotion is any mental experience with high intensity and high hedonic content (pleasure/displeasure).},
	pages = {69--83},
	number = {2},
	journaltitle = {Behavioural Processes},
	shortjournal = {Behavioural Processes},
	author = {Cabanac, Michel},
	urldate = {2020-04-09},
	date = {2002-11-01},
	langid = {english},
	keywords = {Definition, Displeasure, Emotion, Experience, Intensity, Pleasure},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\salam\\Zotero\\storage\\8HIUGMT5\\Cabanac - 2002 - What is emotion.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\salam\\Zotero\\storage\\BQDHICWJ\\S0376635702000785.html:text/html}
}

@article{sabini_ekmans_2005,
	title = {Ekman's basic emotions: Why not love and jealousy?},
	volume = {19},
	issn = {0269-9931, 1464-0600},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02699930441000481},
	doi = {10.1080/02699930441000481},
	shorttitle = {Ekman's basic emotions},
	pages = {693--712},
	number = {5},
	journaltitle = {Cognition \& Emotion},
	shortjournal = {Cognition \& Emotion},
	author = {Sabini, John and Silver, Maury},
	urldate = {2020-04-09},
	date = {2005-08},
	langid = {english},
	file = {Sabini and Silver - 2005 - Ekman's basic emotions Why not love and jealousy.pdf:C\:\\Users\\salam\\Zotero\\storage\\XCWE7FBG\\Sabini and Silver - 2005 - Ekman's basic emotions Why not love and jealousy.pdf:application/pdf}
}

@article{plutchik_psychoevolutionary_1982,
	title = {A psychoevolutionary theory of emotions:},
	url = {https://journals.sagepub.com/doi/10.1177/053901882021004003},
	doi = {10.1177/053901882021004003},
	shorttitle = {A psychoevolutionary theory of emotions},
	journaltitle = {Social Science Information},
	author = {Plutchik, Robert},
	urldate = {2020-04-09},
	date = {1982-07-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications},
	file = {Snapshot:C\:\\Users\\salam\\Zotero\\storage\\AEIZ4K7Q\\053901882021004003.html:text/html}
}

@article{cowen_self-report_2017,
	title = {Self-report captures 27 distinct categories of emotion bridged by continuous gradients},
	rights = {©  . http://www.pnas.org/site/misc/userlicense.xhtml},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/early/2017/08/30/1702247114},
	doi = {10.1073/pnas.1702247114},
	abstract = {Emotions are centered in subjective experiences that people represent, in part, with hundreds, if not thousands, of semantic terms. Claims about the distribution of reported emotional states and the boundaries between emotion categories—that is, the geometric organization of the semantic space of emotion—have sparked intense debate. Here we introduce a conceptual framework to analyze reported emotional states elicited by 2,185 short videos, examining the richest array of reported emotional experiences studied to date and the extent to which reported experiences of emotion are structured by discrete and dimensional geometries. Across self-report methods, we find that the videos reliably elicit 27 distinct varieties of reported emotional experience. Further analyses revealed that categorical labels such as amusement better capture reports of subjective experience than commonly measured affective dimensions (e.g., valence and arousal). Although reported emotional experiences are represented within a semantic space best captured by categorical labels, the boundaries between categories of emotion are fuzzy rather than discrete. By analyzing the distribution of reported emotional states we uncover gradients of emotion—from anxiety to fear to horror to disgust, calmness to aesthetic appreciation to awe, and others—that correspond to smooth variation in affective dimensions such as valence and dominance. Reported emotional states occupy a complex, high-dimensional categorical space. In addition, our library of videos and an interactive map of the emotional states they elicit (https://s3-us-west-1.amazonaws.com/emogifs/map.html) are made available to advance the science of emotion.},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Cowen, Alan S. and Keltner, Dacher},
	urldate = {2020-04-09},
	date = {2017-08-31},
	langid = {english},
	pmid = {28874542},
	note = {Publisher: National Academy of Sciences
Section: {PNAS} Plus},
	keywords = {dimensions, discrete emotion, emotional experience, semantic space},
	file = {Full Text PDF:C\:\\Users\\salam\\Zotero\\storage\\PT2YUP7K\\Cowen and Keltner - 2017 - Self-report captures 27 distinct categories of emo.pdf:application/pdf;Snapshot:C\:\\Users\\salam\\Zotero\\storage\\A9P6H23W\\1702247114.html:text/html}
}

@article{mesquita_cultural_2003,
	title = {Cultural differences in emotions: a context for interpreting emotional experiences},
	volume = {41},
	issn = {00057967},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0005796702001894},
	doi = {10.1016/S0005-7967(02)00189-4},
	shorttitle = {Cultural differences in emotions},
	abstract = {In this article, it is suggested that cross-cultural assessment of emotional disturbances would beneﬁt from the consideration of cultural differences in the modal, and normative emotions. A summary of the research literature on cultural differences in emotions, in particular in antecedent events, subjective feeling, appraisal, and behavior is provided. Cultural differences in emotions are understood functionally, such that the most prevalent emotional phenomena in a culture are those that ﬁt and reinforce the distinct cultural models (i.e. goals and practices) of self and relationship. It is argued that a culture-sensitive approach to emotional disturbances would entail the assessment of emotional phenomena that are dysfunctional to the cultural models of self and relationship.},
	pages = {777--793},
	number = {7},
	journaltitle = {Behaviour Research and Therapy},
	shortjournal = {Behaviour Research and Therapy},
	author = {Mesquita, B and Walker, R},
	urldate = {2020-04-09},
	date = {2003-07},
	langid = {english},
	file = {Mesquita and Walker - 2003 - Cultural differences in emotions a context for in.pdf:C\:\\Users\\salam\\Zotero\\storage\\LJL6PQ85\\Mesquita and Walker - 2003 - Cultural differences in emotions a context for in.pdf:application/pdf}
}

@article{kolakowska_emotion_2014,
	title = {Emotion Recognition and Its Applications},
	volume = {300},
	doi = {10.1007/978-3-319-08491-6_5},
	abstract = {This paper aims at illustrating diversity of possible emotion recognition applications. It provides concise review of affect recognition methods based on different inputs such as biometrics, video channel or behavioral data. It proposes a set of research scenarios of emotion recognition applications in the following domains: software engineering, website customization, education, and gaming. The scenarios show complexity and problems of applying affective computing in different domains. Analysis of the scenarios allows drawing some conclusions on challenges of automatic recognition that have to be addressed by further research.},
	pages = {51--62},
	journaltitle = {Advances in Intelligent Systems and Computing},
	shortjournal = {Advances in Intelligent Systems and Computing},
	author = {Kołakowska, Agata and Landowska, Agnieszka and Szwoch, Mariusz and Szwoch, Wioleta and Wróbel, Michał},
	date = {2014-07-01},
	file = {Full Text PDF:C\:\\Users\\salam\\Zotero\\storage\\53V2W8B5\\Kołakowska et al. - 2014 - Emotion Recognition and Its Applications.pdf:application/pdf}
}

@article{rizzo_human_2005,
	title = {Human Emotional State and its Relevance for Military {VR} Training},
	url = {https://scinapse.io/papers/75706848},
	doi = {null},
	abstract = {Abstract : Combat environments by their nature can produce a dramatic range of emotional responses in military {\textbar} Albert Rizzo, Jacquelyn Ford Morie, Josh Williams,  {\textbar}},
	author = {Rizzo, Albert and Morie, Jacquelyn Ford and Williams, Josh and Pair, Jarrell and Buckwalter, J. G.},
	urldate = {2020-04-09},
	date = {2005-07-01},
	langid = {english},
	file = {Snapshot:C\:\\Users\\salam\\Zotero\\storage\\FI469DNV\\75706848.html:text/html}
}

@inproceedings{rajan_design_2019,
	title = {Design and Development of a Multi-Lingual Speech Corpora ({TaMaR}-{EmoDB}) for Emotion Analysis},
	url = {http://www.isca-speech.org/archive/Interspeech_2019/abstracts/2034.html},
	doi = {10.21437/Interspeech.2019-2034},
	abstract = {This paper presents the design, the development of a new multilingual emotional speech corpus, {TaMaR}- {EmoDB} (Tamil Malayalam Ravula - Emotion {DataBase}) and its evaluation using a deep neural network ({DNN})-baseline system. The corpus consists of utterances from three languages, namely, Malayalam, Tamil and Ravula, a tribal language. The database consists of short speech utterances in four emotions - anger, anxiety, happiness, and sadness, along with neutral utterances. The subset of the corpus is ﬁrst evaluated using a perception test, in order to understand how well the emotional state in emotional speech is identiﬁed by humans. Later, machine testing is performed using the fusion of spectral and prosodic features with {DNN} framework. During the classiﬁcation phase, the system reports an average precision of 0.78, 0.60, 0.61 and recall of 0.84, 0.61 and 0.53 for Malayalam, Tamil, and Ravula, respectively. This database can potentially be used as a new linguistic resource that will enable future research in speech emotion detection, corpus-based prosody analysis, and speech synthesis.},
	eventtitle = {Interspeech 2019},
	pages = {3267--3271},
	booktitle = {Interspeech 2019},
	publisher = {{ISCA}},
	author = {Rajan, Rajeev and G., Haritha U. and C., Sujitha A. and M., Rejisha T.},
	urldate = {2020-04-09},
	date = {2019-09-15},
	langid = {english},
	file = {Rajan et al. - 2019 - Design and Development of a Multi-Lingual Speech C.pdf:C\:\\Users\\salam\\Zotero\\storage\\4Y3FB7WJ\\Rajan et al. - 2019 - Design and Development of a Multi-Lingual Speech C.pdf:application/pdf}
}

@inproceedings{batliner_you_2004,
	location = {Lisbon, Portugal},
	title = {“You Stupid Tin Box” - Children Interacting with the {AIBO} Robot: A Cross-linguistic Emotional Speech Corpus},
	url = {http://www.lrec-conf.org/proceedings/lrec2004/pdf/317.pdf},
	shorttitle = {“You Stupid Tin Box” - Children Interacting with the {AIBO} Robot},
	eventtitle = {{LREC} 2004},
	booktitle = {Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}'04)},
	publisher = {European Language Resources Association ({ELRA})},
	author = {Batliner, A. and Hacker, C. and Steidl, S. and Nöth, E. and D'Arcy, S. and Russell, M. and Wong, M.},
	urldate = {2020-04-09},
	date = {2004-05},
	file = {Full Text PDF:C\:\\Users\\salam\\Zotero\\storage\\H9K7LU7Z\\Batliner et al. - 2004 - “You Stupid Tin Box” - Children Interacting with t.pdf:application/pdf}
}

@article{russell_pf-star_2006,
	title = {The {PF}-{STAR} British English Children’s Speech Corpus},
	abstract = {The {PF}-{STAR} British English children’s speech corpus was collected as part of {EU} Framework 5 project {IST}-2001-37599 ‘{PF}-{STAR}: Preparing for Future Multisensorial Interaction Research’. The corpus was collected at three location, a university laboratory and two primary schools, by researchers at the Department of Electronic, Electrical and Computer Engineering at the University of Birmingham, {UK}. This corpus contains speech from 158 children aged 4 to 14 years. The majority of the children (excluding some of the younger children) recorded 20 ‘{SCRIBE}’ sentences, a list of 40 isolated words, a list of 10 ‘phonetically rich’ sentences, 20 ‘generic phrases’, an ‘accent diagnostic’ passage (the ‘sailor passage’) and a list of 20 digit triples. The recordings are divided into a training set (86 speakers, 703 recorded speech ﬁles, 7 hrs 29 mins 49.030 secs including non-speech), evaluation set (12 speakers, 97 recorded speech ﬁles, 53 mins 57.579 secs including non-speech) and test set (60 speakers, 510 recorded speech ﬁles, 5 hrs 49 mins 47.088 secs including non-speech).},
	pages = {35},
	author = {Russell, Martin},
	date = {2006-12-14},
	langid = {english},
	file = {Russell - The PF-STAR British English Children’s Speech Corp.pdf:C\:\\Users\\salam\\Zotero\\storage\\U2DUGWT5\\Russell - The PF-STAR British English Children’s Speech Corp.pdf:application/pdf}
}

@inproceedings{batliner_pf_star_2005,
	title = {The {PF}\_STAR children's speech corpus.},
	abstract = {This paper describes the corpus of recordings of children's speech which was collected as part of the {EU} {FP}5 {PF} {STAR} project. The corpus contains more than 60 hours of speech, including read and imitated native-language speech in British English, German and Swedish, read and imitated non-native- language English speech from German, Italian and Swedish children, and native-language spontaneous and emotional speech in English and German.},
	pages = {2761--2764},
	author = {Batliner, Anton and Blomberg, Mats and D'Arcy, Shona and Elenius, Daniel and Giuliani, Diego and Gerosa, Matteo and Hacker, Christian and Russell, Martin and Steidl, Stefan and Wong, Michael},
	date = {2005-01-01},
	file = {Full Text PDF:C\:\\Users\\salam\\Zotero\\storage\\GQBD3JER\\Batliner et al. - 2005 - The PF_STAR children's speech corpus..pdf:application/pdf}
}

@article{perez-espinosa_iesc-child_2020,
	title = {{IESC}-Child: An Interactive Emotional Children’s Speech Corpus},
	volume = {59},
	issn = {0885-2308},
	url = {http://www.sciencedirect.com/science/article/pii/S0885230817301547},
	doi = {10.1016/j.csl.2019.06.006},
	shorttitle = {{IESC}-Child},
	abstract = {In this paper, we describe the process that we used to create a new corpus of children’s emotional speech. We used a Wizard of Oz ({WoZ}) setting to induce different emotional reactions in children during speech-based interactions with two robots. We recorded the speech spoken in Mexican Spanish by 174 children (both sexes) between 6 and 11 years of age. The recordings were manually segmented and transcribed. The segments were then labeled with two types of emotional-related paralinguistic information: emotion and attitude. The corpus contained 2093 min of audio recordings (34.88 h) divided into 19,793 speech segments. The Interactive Emotional Children’s Speech Corpus ({IESC}-Child) can be a valuable resource for researchers studying affective reactions in speech communication during child-computer interactions in Spanish and for creating models to recognize acoustic paralinguistic information. {IESC}-Child is available to the research community upon request.},
	pages = {55--74},
	journaltitle = {Computer Speech \& Language},
	shortjournal = {Computer Speech \& Language},
	author = {Pérez-Espinosa, Humberto and Martínez-Miranda, Juan and Espinosa-Curiel, Ismael and Rodríguez-Jacobo, Josefina and Villaseñor-Pineda, Luis and Avila-George, Himer},
	urldate = {2020-04-09},
	date = {2020-01-01},
	langid = {english},
	keywords = {Emotional analysis, Interactive systems, Paralinguistic information},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\salam\\Zotero\\storage\\9FRD3CX7\\Pérez-Espinosa et al. - 2020 - IESC-Child An Interactive Emotional Children’s Sp.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\salam\\Zotero\\storage\\JTWW5AMR\\S0885230817301547.html:text/html}
}

@inproceedings{jin_speech_2015,
	title = {Speech emotion recognition with acoustic and lexical features},
	doi = {10.1109/ICASSP.2015.7178872},
	abstract = {In this paper we explore one of the key aspects in building an emotion recognition system: generating suitable feature representations. We generate feature representations from both acoustic and lexical levels. At the acoustic level, we first extract low-level features such as intensity, F0, jitter, shimmer and spectral contours etc. We then generate different acoustic feature representations based on these low-level features, including statistics over these features, a new representation derived from a set of low-level acoustic codewords, and a new representation from Gaussian Supervectors. At the lexical level, we propose a new feature representation named emotion vector ({eVector}). We also use the traditional Bag-of-Words ({BoW}) feature. We apply these feature representations for emotion recognition and compare their performance on the {USC}-{IEMOCAP} database. We also combine these different feature representations via early fusion and late fusion. Our experimental results show that late fusion of both acoustic and lexical features achieves four-class emotion recognition accuracy of 69.2\%.},
	eventtitle = {2015 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {4749--4753},
	booktitle = {2015 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Jin, Qin and Li, Chengxin and Chen, Shizhe and Wu, Huimin},
	date = {2015-04},
	note = {{ISSN}: 2379-190X},
	keywords = {Accuracy, acoustic feature representations, acoustic features, Acoustic features, Acoustics, bag of words feature, computational linguistics, Emotion lexicon, emotion recognition, Emotion recognition, emotion recognition system, emotion vector, {eVector}, Feature extraction, Gaussian supervectors, lexical feature representation, lexical features, Lexical features, low level acoustic codewords, low level features, signal representation, Speech, speech emotion recognition, speech fundamental frequency, speech intensity, speech jitter, speech processing, speech recognition, Speech recognition, speech shimmer, speech spectral contours, Support vector machine, Support vector machines, {USC}-{IEMOCAP} database},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\salam\\Zotero\\storage\\36RQQMB9\\7178872.html:text/html}
}

@article{frick_communicating_1985,
	title = {Communicating emotion: The role of prosodic features},
	volume = {97},
	issn = {1939-1455(Electronic),0033-2909(Print)},
	doi = {10.1037/0033-2909.97.3.412},
	shorttitle = {Communicating emotion},
	abstract = {Reviews research on the expression of emotion through the nonverbal (prosodic) features of speech. Findings show that emotions can be expressed prosodically, apparently through a variety of prosodic features. This communication appears to be largely the same for different individuals and cultures, suggesting that the prosodic expression of emotion is not conventional. Some correlations between dimensions of emotions (e.g., anxiety, aggression) and prosodic features are discussed; activity or arousal seems to be signaled by increased pitch height, pitch range, loudness, and rate. The possibility that prosodic contours (patterns of pitch and loudness over time) are used to communicate specific emotions is explored. A number of authors suggest that anger is communicated by an even contour with occasional sharp increases in pitch and loudness. Methodological difficulties with the acoustical manipulation of relevant auditory and articulatory features are noted. It is suggested that a major step in investigating the prosodic expression of emotion will be learning how to synthesize various articulatory and auditory features. (3 p ref) ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {412--429},
	number = {3},
	journaltitle = {Psychological Bulletin},
	author = {Frick, Robert W.},
	date = {1985},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Emotions, Literature Review, Oral Communication, Speech Characteristics},
	file = {Snapshot:C\:\\Users\\salam\\Zotero\\storage\\8YIIEVCU\\1985-22244-001.html:text/html}
}

@inproceedings{alex_utterance_2018,
	title = {Utterance and Syllable Level Prosodic Features for Automatic Emotion Recognition},
	doi = {10.1109/RAICS.2018.8635059},
	abstract = {This paper describes an automatic emotion recognition ({AER}) system that combines prosodic features extracted at utterance level and syllable level to recognize its emotional content. The prosodic features are extracted after identifying speech/non-speech intervals, followed by syllable level segmentation. Prosodic features chosen include parameters for representing dynamics of pitch and energy, along with duration information. Two separate classifiers are built using Deep Neural Networks ({DNN}). The decision scores based on both levels are fused to identify the emotion of a test utterance from the German Emotion Database (Emo-{DB}) which contains seven emotions, namely anger, boredom, disgust, fear, happiness, sadness and neutral. The proposed system gives a Weighted Average Recall ({WAR}) of 58.88\% for both utterance level and syllable level prosodic features. Fusion of scores by merely adding the scores gives an overall {WAR} of 61.68\%.},
	eventtitle = {2018 {IEEE} Recent Advances in Intelligent Computational Systems ({RAICS})},
	pages = {31--35},
	booktitle = {2018 {IEEE} Recent Advances in Intelligent Computational Systems ({RAICS})},
	author = {Alex, Starlet Ben and Babu, Ben P. and Mary, Leena},
	date = {2018-12},
	keywords = {{AER} system, automatic emotion recognition, automatic emotion recognition system, deep neural networks, {DNN}, Emo-{DB}, emotion recognition, feature extraction, German emotion database, neural nets, prosodic features, speech processing, speech recognition, speech-nonspeech intervals, syllable level segmentation, {WAR}, weighted average recall},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\salam\\Zotero\\storage\\U2QP4LIH\\8635059.html:text/html}
}

@article{alim_commonly_2018,
	title = {Some Commonly Used Speech Feature Extraction Algorithms},
	url = {https://www.intechopen.com/books/from-natural-to-artificial-intelligence-algorithms-and-applications/some-commonly-used-speech-feature-extraction-algorithms},
	doi = {10.5772/intechopen.80419},
	abstract = {Speech is a complex naturally acquired human motor ability. It is characterized in adults with the production of about 14 different sounds per second via the harmonized actions of roughly 100 muscles. Speaker recognition is the capability of a software or hardware to receive speech signal, identify the speaker present in the speech signal and recognize the speaker afterwards. Feature extraction is accomplished by changing the speech waveform to a form of parametric representation at a relatively minimized data rate for subsequent processing and analysis. Therefore, acceptable classification is derived from excellent and quality features. Mel Frequency Cepstral Coefficients ({MFCC}), Linear Prediction Coefficients ({LPC}), Linear Prediction Cepstral Coefficients ({LPCC}), Line Spectral Frequencies ({LSF}), Discrete Wavelet Transform ({DWT}) and Perceptual Linear Prediction ({PLP}) are the speech feature extraction techniques that were discussed in these chapter. These methods have been tested in a wide variety of applications, giving them high level of reliability and acceptability. Researchers have made several modifications to the above discussed techniques to make them less susceptible to noise, more robust and consume less time. In conclusion, none of the methods is superior to the other, the area of application would determine which method to select.},
	journaltitle = {From Natural to Artificial Intelligence - Algorithms and Applications},
	author = {Alim, Sabur Ajibola and Rashid, Nahrul Khair Alang},
	urldate = {2020-04-09},
	date = {2018-12-12},
	langid = {english},
	note = {Publisher: {IntechOpen}},
	file = {Full Text PDF:C\:\\Users\\salam\\Zotero\\storage\\749EQ8VH\\Alim and Rashid - 2018 - Some Commonly Used Speech Feature Extraction Algor.pdf:application/pdf;Snapshot:C\:\\Users\\salam\\Zotero\\storage\\T55VBLHU\\some-commonly-used-speech-feature-extraction-algorithms.html:text/html}
}

@online{gupta_application_2016,
	title = {Application of {MFCC} in Text Independent Speaker Recognition},
	url = {https://www.semanticscholar.org/paper/Application-of-MFCC-in-Text-Independent-Speaker-Gupta/2aa9c2971342e8b0b1a0714938f39c406f258477},
	abstract = {Recently speech processing is one of the important application area of digital signal processing. There are several parts of speech processing as speech recognition, speaker recognition, speech synthesis, speech coding etc. The objective of the presented work is to extract, characterize and recognize the speaker identity. Feature extraction is the key process for speaker recognition. In this work, the Mel Frequency Cepstrum Coefficient ({MFCC}) feature has been utilized for designing a speaker identification system which is independent of speech rather than previously reported text dependent techniques.},
	author = {Gupta, Shipra},
	urldate = {2020-04-09},
	date = {2016},
	langid = {english},
	note = {Library Catalog: www.semanticscholar.org},
	file = {Snapshot:C\:\\Users\\salam\\Zotero\\storage\\C3YBTS37\\2aa9c2971342e8b0b1a0714938f39c406f258477.html:text/html}
}

@article{reddy_audio_2020,
	title = {Audio compression with multi-algorithm fusion and its impact in speech emotion recognition},
	issn = {1572-8110},
	url = {https://doi.org/10.1007/s10772-020-09689-9},
	doi = {10.1007/s10772-020-09689-9},
	abstract = {The study examines the impact of multi-algorithm fusion over audio compression with reference to the traditional exercises. For emotion recognition, here the most prominent features ‘Mel Frequency Cepstral Coefficients’ ({MFCC}) and ‘Discrete Wavelet Transform’ ({DWT}) features are extracted from prevalent speech samples of Berlin emotional database and Telugu (a south Indian language) database, we proposed automatic emotion recognition system ({AERS}) based on multi-algorithms fusion. {AERS} means to monitor and identify unit psychological/emotional state. The extracted features are analyzed using support vector machine, K-{NN} algorithms used for the classification of different states of emotion. Using two state-of-art mp3, Speex codec with different bit-rates investigated to ensure specific emotional intelligibility. {MP}3 codec configuration with 96   kbps bit-rate is recommended to pull off high compression for all emotions. Fusion algorithms also performed well compared with individual algorithms. Accuracy of 94.2\% using fusion {DWT} and {MFCC} compared to 89.1\% using {DWT} and 91.38\% using {MFCC} separately. The accuracy of the proposed method increased further to 94\% through a multiresolution approach by approximating frequency along with time information.},
	journaltitle = {International Journal of Speech Technology},
	shortjournal = {Int J Speech Technol},
	author = {Reddy, A. Pramod and Vijayarajan, V.},
	urldate = {2020-04-09},
	date = {2020-02-27},
	langid = {english},
	file = {Springer Full Text PDF:C\:\\Users\\salam\\Zotero\\storage\\SGWLZBCM\\Reddy and Vijayarajan - 2020 - Audio compression with multi-algorithm fusion and .pdf:application/pdf}
}

@inproceedings{al-kaltakchi_comparison_2017,
	title = {Comparison of I-vector and {GMM}-{UBM} approaches to speaker identification with {TIMIT} and {NIST} 2008 databases in challenging environments},
	doi = {10.23919/EUSIPCO.2017.8081264},
	abstract = {In this paper, two models, the I-vector and the Gaussian Mixture Model-Universal Background Model ({GMM}-{UBM}), are compared for the speaker identification task. Four feature combinations of I-vectors with seven fusion techniques are considered: maximum, mean, weighted sum, cumulative, interleaving and concatenated for both two and four features. In addition, an Extreme Learning Machine ({ELM}) is exploited to identify speakers, and then Speaker Identification Accuracy ({SIA}) is calculated. Both systems are evaluated for 120 speakers from the {TIMIT} and {NIST} 2008 databases for clean speech. Furthermore, a comprehensive evaluation is made under Additive White Gaussian Noise ({AWGN}) conditions and with three types of Non Stationary Noise ({NSN}), both with and without handset effects for the {TIMIT} database. The results show that the I-vector approach is better than the {GMM}-{UBM} for both clean and {AWGN} conditions without a handset. However, the {GMM}-{UBM} had better accuracy for {NSN} types.},
	eventtitle = {2017 25th European Signal Processing Conference ({EUSIPCO})},
	pages = {533--537},
	booktitle = {2017 25th European Signal Processing Conference ({EUSIPCO})},
	author = {Al-Kaltakchi, Musab T. S. and Woo, Wai L. and Dlay, Satnam S. and Chambers, Jonathon A.},
	date = {2017-08},
	note = {{ISSN}: 2076-1465},
	keywords = {additive white Gaussian noise conditions, {AWGN}, {AWGN} conditions, Databases, extreme learning machine, Gaussian mixture model-universal background model, Gaussian processes, {GMM}-{UBM} approaches, learning (artificial intelligence), Mel frequency cepstral coefficient, {NIST}, Noise measurement, speaker identification accuracy, speaker identification task, speaker recognition, Speech, Telephone sets, Training},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\salam\\Zotero\\storage\\CABDCEH5\\8081264.html:text/html;Submitted Version:C\:\\Users\\salam\\Zotero\\storage\\CRMZFAV7\\Al-Kaltakchi et al. - 2017 - Comparison of I-vector and GMM-UBM approaches to s.pdf:application/pdf}
}

@article{kinnunen_overview_2010,
	title = {An Overview of Text-Independent Speaker Recognition: from Features to Supervectors},
	volume = {52},
	doi = {10.1016/j.specom.2009.08.009},
	shorttitle = {An Overview of Text-Independent Speaker Recognition},
	abstract = {This paper gives an overview of automatic speaker recognition technology, with an emphasis on text-independent recognition. Speaker recognition has been studied actively for several decades. We give an overview of both the classical and the state-of-the-art methods. We start with the fundamentals of automatic speaker recognition, concerning feature extraction and speaker modeling. We elaborate advanced computational techniques to address robustness and session variability. The recent progress from vectors towards supervectors opens up a new area of exploration and represents a technology trend. We also provide an overview of this recent development and discuss the evaluation methodology of speaker recognition systems. We conclude the paper with discussion on future directions.},
	pages = {12--40},
	journaltitle = {Speech Communication},
	shortjournal = {Speech Communication},
	author = {Kinnunen, Tomi and Li, Haizhou},
	date = {2010-01-31},
	file = {Full Text PDF:C\:\\Users\\salam\\Zotero\\storage\\SM2J9K6E\\Kinnunen and Li - 2010 - An Overview of Text-Independent Speaker Recognitio.pdf:application/pdf}
}

@inproceedings{hu_gmm_2007,
	title = {{GMM} Supervector Based {SVM} with Spectral Features for Speech Emotion Recognition},
	volume = {4},
	doi = {10.1109/ICASSP.2007.366937},
	abstract = {Speech emotion recognition is a challenging yet important speech technology. In this paper, the {GMM} supervector based {SVM} is applied to this field with spectral features. A {GMM} is trained for each emotional utterance, and the corresponding {GMM} supervector is used as the input feature for {SVM}. Experimental results on an emotional speech database demonstrate that the {GMM} supervector based {SVM} outperforms standard {GMM} on speech emotion recognition.},
	eventtitle = {2007 {IEEE} International Conference on Acoustics, Speech and Signal Processing - {ICASSP} '07},
	pages = {IV--413--IV--416},
	booktitle = {2007 {IEEE} International Conference on Acoustics, Speech and Signal Processing - {ICASSP} '07},
	author = {Hu, Hao and Xu, Ming-Xing and Wu, Wei},
	date = {2007-04},
	note = {{ISSN}: 2379-190X},
	keywords = {Cepstral analysis, emotion recognition, Emotion recognition, emotional speech database, feature extraction, Feature extraction, Gaussian processes, {GMM} supervector, Hidden Markov models, Mel frequency cepstral coefficient, Spatial databases, Speaker recognition, spectral features, Speech, speech emotion recognition, Speech emotion recognition, speech processing, speech recognition, Support vector machine classification, support vector machines, Support vector machines, {SVM}},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\salam\\Zotero\\storage\\KRMK6U26\\4218125.html:text/html}
}

@article{markel_long-term_1977,
	title = {Long-term feature averaging for speaker recognition},
	volume = {25},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1977.1162961},
	abstract = {The potential benefits of long-term parameter averaging for speaker recognition were investigated. Parameters studied were pitch, gain, and reflection coefficients. Parameter variability was computed over various averaging lengths from one frame averaging (in effect, no averaging) to 1000 frame averaging (about 70 s of speech). It was demonstrated that the between-to-within speaker variance ratio, measured over several speakers, was significantly increased by performing long-term averaging of the parameter sets. The reflection coefficient averages for k2and k6, respectively, were shown to produce the highest variance ratios.},
	pages = {330--337},
	number = {4},
	journaltitle = {{IEEE} Transactions on Acoustics, Speech, and Signal Processing},
	author = {Markel, J. and Oshika, B. and Gray, A.},
	date = {1977-08},
	note = {Conference Name: {IEEE} Transactions on Acoustics, Speech, and Signal Processing},
	keywords = {Analysis of variance, Autocorrelation, Equations, Frequency estimation, Loudspeakers, Oral communication, Reflection, Speaker recognition, Speech analysis, Speech synthesis},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\salam\\Zotero\\storage\\3MFJM2HD\\1162961.html:text/html}
}

@article{karafiat_ivector-based_2011,
	title = {{iVector}-based discriminative adaptation for automatic speech recognition},
	doi = {10.1109/ASRU.2011.6163922},
	abstract = {We presented a novel technique for discriminative feature-level adaptation of automatic speech recognition system. The concept of {iVectors} popular in Speaker Recognition is used to extract information about speaker or acoustic environment from speech segment. {iVector} is a low-dimensional fixed-length representing such information. To utilized {iVectors} for adaptation, Region Dependent Linear Transforms ({RDLT}) are discriminatively trained using {MPE} criterion on large amount of annotated data to extract the relevant information from {iVectors} and to compensate speech feature. The approach was tested on standard {CTS} data. We found it to be complementary to common adaptation techniques. On a well tuned {RDLT} system with standard {CMLLR} adaptation we reached 0.8\% additive absolute {WER} improvement.},
	author = {Karafiát, Martin and Burget, Lukas and Matejka, Pavel and Glembek, Ondrej and Cernocky, Jan},
	date = {2011-12-01},
	file = {Full Text PDF:C\:\\Users\\salam\\Zotero\\storage\\HCI9D2AX\\Karafiát et al. - 2011 - iVector-based discriminative adaptation for automa.pdf:application/pdf}
}

@article{ibrahim_i-vector_2018,
	title = {I-vector Extraction for Speaker Recognition Based on Dimensionality Reduction},
	volume = {126},
	issn = {1877-0509},
	url = {http://www.sciencedirect.com/science/article/pii/S1877050918314042},
	doi = {10.1016/j.procs.2018.08.126},
	series = {Knowledge-Based and Intelligent Information \& Engineering Systems: Proceedings of the 22nd International Conference, {KES}-2018, Belgrade, Serbia},
	abstract = {In the domain of speaker recognition, many methods have been proposed over time. The technology for automatic speaker recognition has now reached a good level of performance but there is still need of improvement. In this paper, a new low-dimensional speaker- and channel-dependent space is defined using a simple factor analysis also known as i-vector. This space is named the total variability space because it models both speaker and channel variabilities. The i-vector subspace modelling is one of the recent methods that have become the state of the art technique in this domain. This method largely provides the benefit of modelling both the intra-domain and inter-domain variabilities into the same low dimensional space. In this study, 2656 syllables bio-acoustic signals from 55 species of frog taken from Intelligent Biometric Group, {USM} database are used for frog identification system. Parameters of the system are initially tuned such as Universal Background Model ({UBM}) size (32, 64 and 128 Gaussians) and i-vector dimensionality (100, 200 and 400 dimensions). To the end, we assess the effect of the parameter tuned and record the computation time. We observed that, the accuracy for smaller {UBM} size and higher i-vector dimensionality outperforms others with result of 91.11\% is achieved. From this research, it can be concluded that {UBM} size and i-vector dimensionality effect the accuracy of frog identification based on i-vector.},
	pages = {1534--1540},
	journaltitle = {Procedia Computer Science},
	shortjournal = {Procedia Computer Science},
	author = {Ibrahim, Noor Salwani and Ramli, Dzati Athiar},
	urldate = {2020-04-09},
	date = {2018-01-01},
	langid = {english},
	keywords = {Bob Spear toolbox, Dimensionality Reduction, Frog Identification, I-vectors, {UBM} size},
	file = {Full Text:C\:\\Users\\salam\\Zotero\\storage\\TW9R3X7K\\Ibrahim and Ramli - 2018 - I-vector Extraction for Speaker Recognition Based .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\salam\\Zotero\\storage\\HC9WYPSG\\S1877050918314042.html:text/html}
}

@article{chen_support_2016,
	title = {Support Super-Vector Machines in Automatic Speech Emotion Recognition},
	abstract = {In this paper, we use super-vectors in support vector machines for automatic speech emotion recognition. In our implementation, an utterance is converted to a super-vector formed by the mean vectors of a Gaussian mixture model adapted from a universal background model. The proposed method is evaluated on {FAU}-Aibo database which is wellknown to be used in {INTERSPEECH} 2009 Emotion Challenge. In the case of {HMMbased} dynamic modeling classiﬁer, we achieve an unweighted average ({UA}) recall rate of 40.0\%, over a baseline of 35.5\%, by using the delta features and increasing the number of mixture components. In the case of {SVM}-based static modeling classiﬁer, we achieve an unweighted average ({UA}) recall rate of 38.9\%, over a baseline of 38.2\%, by using the proposed super-vectors.},
	pages = {11},
	author = {Chen, Chia-Ying and Chen, Chia-Ping},
	date = {2016},
	langid = {english},
	file = {Chen and Chen - Support Super-Vector Machines in Automatic Speech .pdf:C\:\\Users\\salam\\Zotero\\storage\\X2MN8R34\\Chen and Chen - Support Super-Vector Machines in Automatic Speech .pdf:application/pdf}
}

@inproceedings{casale_speech_2008,
	title = {Speech Emotion Classification Using Machine Learning Algorithms},
	doi = {10.1109/ICSC.2008.43},
	abstract = {The recognition of emotional states is a relatively new technique in the field of machine learning. The paper presents the study and the performance results of a system for emotion classification using the architecture of a distributed speech recognition system ({DSR}). The features used were extracted by the front-end {ETSI} Aurora {eXtended} of a mobile terminal in compliance with the {ETSI} {ES} 202-211 V1.1.1 standard. On the basis of the time trend of these parameters, over 3800 statistical parameters were extracted to characterize semantic units of varying length (sentences and words). Using the {WEKA} (Waikato Environment for Knowledge Analysis) software the most significant parameters for the classification of emotional states were selected and the results of various classification techniques were analysed. The results, obtained using both the Berlin Database of Emotional Speech ({EMO}-{DB}) and the Speech Under Simulated and Actual Stress ({SUSAS}) corpus, showed that the best performance is achieved using a support vector machine ({SVM}) trained with the sequential minimal optimization ({SMO}) algorithm, after normalizing and discretizing the input statistical parameters.},
	eventtitle = {2008 {IEEE} International Conference on Semantic Computing},
	pages = {158--165},
	booktitle = {2008 {IEEE} International Conference on Semantic Computing},
	author = {Casale, S. and Russo, A. and Scebba, G. and Serrano, S.},
	date = {2008-08},
	keywords = {Aggregates, Berlin Database of Emotional Speech, Classification algorithms, Databases, distributed speech recognition system, emo-db, emotion classification, {ETSI} {ES} 202-211 standard, Feature extraction, learning (artificial intelligence), machine learning, machine learning algorithm, optimisation, pattern classification, sentences, sequential minimal optimization algorithm, Speech, speech analysis, speech emotion classification, speech recognition, Speech recognition, Speech Under Simulated and Actual Stress, Stress, support vector machine, support vector machines, susas, {SVM}, Waikato Environment for Knowledge Analysis, {WEKA}, words},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\salam\\Zotero\\storage\\UNGRJRS4\\4597187.html:text/html}
}

@article{mehta_recognition_2019,
	title = {Recognition of Emotion Intensities Using Machine Learning Algorithms: A Comparative Study},
	volume = {19},
	issn = {1424-8220},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6514572/},
	doi = {10.3390/s19081897},
	shorttitle = {Recognition of Emotion Intensities Using Machine Learning Algorithms},
	abstract = {Over the past two decades, automatic facial emotion recognition has received enormous attention. This is due to the increase in the need for behavioral biometric systems and human–machine interaction where the facial emotion recognition and the intensity of emotion play vital roles. The existing works usually do not encode the intensity of the observed facial emotion and even less involve modeling the multi-class facial behavior data jointly. Our work involves recognizing the emotion along with the respective intensities of those emotions. The algorithms used in this comparative study are Gabor filters, a Histogram of Oriented Gradients ({HOG}), and Local Binary Pattern ({LBP}) for feature extraction. For classification, we have used Support Vector Machine ({SVM}), Random Forest ({RF}), and Nearest Neighbor Algorithm ({kNN}). This attains emotion recognition and intensity estimation of each recognized emotion. This is a comparative study of classifiers used for facial emotion recognition along with the intensity estimation of those emotions for databases. The results verified that the comparative study could be further used in real-time behavioral facial emotion and intensity of emotion recognition.},
	number = {8},
	journaltitle = {Sensors (Basel, Switzerland)},
	shortjournal = {Sensors (Basel)},
	author = {Mehta, Dhwani and Siddiqui, Mohammad Faridul Haque and Javaid, Ahmad Y.},
	urldate = {2020-04-09},
	date = {2019-04-21},
	pmid = {31010081},
	pmcid = {PMC6514572},
	file = {PubMed Central Full Text PDF:C\:\\Users\\salam\\Zotero\\storage\\BWKZVKNQ\\Mehta et al. - 2019 - Recognition of Emotion Intensities Using Machine L.pdf:application/pdf}
}

@article{han_speech_2014,
	title = {Speech Emotion Recognition Using Deep Neural Network and Extreme Learning Machine},
	abstract = {Speech emotion recognition is a challenging problem partly because it is unclear what features are effective for the task. In this paper we propose to utilize deep neural networks ({DNNs}) to extract high level features from raw data and show that they are effective for speech emotion recognition. We ﬁrst produce an emotion state probability distribution for each speech segment using {DNNs}. We then construct utterance-level features from segment-level probability distributions. These utterancelevel features are then fed into an extreme learning machine ({ELM}), a special simple and efﬁcient single-hidden-layer neural network, to identify utterance-level emotions. The experimental results demonstrate that the proposed approach effectively learns emotional information from low-level features and leads to 20\% relative accuracy improvement compared to the stateof-the-art approaches.},
	pages = {5},
	author = {Han, Kun and Yu, Dong and Tashev, Ivan},
	date = {2014},
	langid = {english},
	file = {Han et al. - Speech Emotion Recognition Using Deep Neural Netwo.pdf:C\:\\Users\\salam\\Zotero\\storage\\NLLERAEC\\Han et al. - Speech Emotion Recognition Using Deep Neural Netwo.pdf:application/pdf}
}

@inproceedings{lim_speech_2016,
	title = {Speech emotion recognition using convolutional and Recurrent Neural Networks},
	doi = {10.1109/APSIPA.2016.7820699},
	abstract = {With rapid developments in the design of deep architecture models and learning algorithms, methods referred to as deep learning have come to be widely used in a variety of research areas such as pattern recognition, classification, and signal processing. Deep learning methods are being applied in various recognition tasks such as image, speech, and music recognition. Convolutional Neural Networks ({CNNs}) especially show remarkable recognition performance for computer vision tasks. In addition, Recurrent Neural Networks ({RNNs}) show considerable success in many sequential data processing tasks. In this study, we investigate the result of the Speech Emotion Recognition ({SER}) algorithm based on {CNNs} and {RNNs} trained using an emotional speech database. The main goal of our work is to propose a {SER} method based on concatenated {CNNs} and {RNNs} without using any traditional hand-crafted features. By applying the proposed methods to an emotional speech database, the classification result was verified to have better accuracy than that achieved using conventional classification methods.},
	eventtitle = {2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference ({APSIPA})},
	pages = {1--4},
	booktitle = {2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference ({APSIPA})},
	author = {Lim, Wootaek and Jang, Daeyoung and Lee, Taejin},
	date = {2016-12},
	keywords = {computer vision, Convolution, convolutional neural network, Databases, deep architecture model design, deep learning algorithm, emotion recognition, Emotion recognition, emotional speech database, learning (artificial intelligence), recognition performance, recurrent neural nets, recurrent neural network, Recurrent neural networks, sequential data processing, Speech, speech emotion recognition, speech recognition, Speech recognition},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\salam\\Zotero\\storage\\KZREBE7C\\7820699.html:text/html}
}

@article{issa_speech_2020,
	title = {Speech emotion recognition with deep convolutional neural networks},
	volume = {59},
	issn = {1746-8094},
	url = {http://www.sciencedirect.com/science/article/pii/S1746809420300501},
	doi = {10.1016/j.bspc.2020.101894},
	abstract = {The speech emotion recognition (or, classification) is one of the most challenging topics in data science. In this work, we introduce a new architecture, which extracts mel-frequency cepstral coefficients, chromagram, mel-scale spectrogram, Tonnetz representation, and spectral contrast features from sound files and uses them as inputs for the one-dimensional Convolutional Neural Network for the identification of emotions using samples from the Ryerson Audio-Visual Database of Emotional Speech and Song ({RAVDESS}), Berlin ({EMO}-{DB}), and Interactive Emotional Dyadic Motion Capture ({IEMOCAP}) datasets. We utilize an incremental method for modifying our initial model in order to improve classification accuracy. All of the proposed models work directly with raw sound data without the need for conversion to visual representations, unlike some previous approaches. Based on experimental results, our best-performing model outperforms existing frameworks for {RAVDESS} and {IEMOCAP}, thus setting the new state-of-the-art. For the {EMO}-{DB} dataset, it outperforms all previous works except one but compares favorably with that one in terms of generality, simplicity, and applicability. Specifically, the proposed framework obtains 71.61\% for {RAVDESS} with 8 classes, 86.1\% for {EMO}-{DB} with 535 samples in 7 classes, 95.71\% for {EMO}-{DB} with 520 samples in 7 classes, and 64.3\% for {IEMOCAP} with 4 classes in speaker-independent audio classification tasks.},
	pages = {101894},
	journaltitle = {Biomedical Signal Processing and Control},
	shortjournal = {Biomedical Signal Processing and Control},
	author = {Issa, Dias and Fatih Demirci, M. and Yazici, Adnan},
	urldate = {2020-04-09},
	date = {2020-05-01},
	langid = {english},
	keywords = {Deep learning, Signal processing, Speech emotion recognition},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\salam\\Zotero\\storage\\VS7SN595\\Issa et al. - 2020 - Speech emotion recognition with deep convolutional.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\salam\\Zotero\\storage\\BMGHF628\\S1746809420300501.html:text/html}
}

@inproceedings{gao_end--end_2019,
	location = {Suzhou, China},
	title = {End-to-End Speech Emotion Recognition Based on One-Dimensional Convolutional Neural Network},
	isbn = {978-1-4503-6128-6},
	url = {https://doi.org/10.1145/3319921.3319963},
	doi = {10.1145/3319921.3319963},
	series = {{ICIAI} 2019},
	abstract = {Real-time speech emotion recognition has always been a problem. To this end, we proposed an end-to-end speech emotion recognition model based on one-dimensional convolutional neural network, which contains only three convolution layers, two pooling layers and one full-connected layer. Through Adam optimization algorithm and back propagation mechanism, more discriminative features can be extracted continuously. Our model is quite simple in structure and easy to quickly complete the emotional classification task. Compared with traditional methods, there is no need to carry out the complex process of manually extracting features, and the model can automatically learn the emotional features from raw speech signals. In the emotional recognition experiments with {EMODB}, {CASIA}, {IEMOCAP}, and {CHEAVD} four speech databases, relatively high recognition rates were obtained. Experiments show that the proposed algorithm is of great benefit to the implementation of real-time speech emotion recognition.},
	pages = {78--82},
	booktitle = {Proceedings of the 2019 3rd International Conference on Innovation in Artificial Intelligence},
	publisher = {Association for Computing Machinery},
	author = {Gao, Mengna and Dong, Jing and Zhou, Dongsheng and Zhang, Qiang and Yang, Deyun},
	urldate = {2020-04-09},
	date = {2019-03-15},
	keywords = {Convolutional Neural Network, End-to-End, Speech Emotion Recognition},
	file = {Full Text PDF:C\:\\Users\\salam\\Zotero\\storage\\MN6J55DG\\Gao et al. - 2019 - End-to-End Speech Emotion Recognition Based on One.pdf:application/pdf}
}

@incollection{batliner_automatic_2011,
	title = {The Automatic Recognition of Emotions in Speech},
	abstract = {In this chapter, we focus on the automatic recognition of emotional states using acoustic and linguistic parameters as features and classifiers as tools to predict the ‘correct’ emotional states. We first sketch history and state of the art in this field; then we describe the process of ‘corpus engineering’, i.e. the design and the recording of databases, the annotation of emotional states, and further processing such as manual or automatic segmentation. Next, we present an overview of acoustic and linguistic features that are extracted automatically or manually. In the section on classifiers, we deal with topics such as the curse of dimensionality and the sparse data problem, classifiers, and evaluation. At the end of each section, we point out important aspects that should be taken into account for the planning or the assessment of studies. The subject area of this chapter is not emotions in some narrow sense but in a wider sense encompassing emotion-related states such as moods, attitudes, or interpersonal stances as well. We do not aim at an in-depth treatise of some specific aspects or algorithms but at an overview of approaches and strategies that have been used or should be used.},
	pages = {71--99},
	booktitle = {Cognitive Technologies},
	author = {Batliner, Anton and Schuller, Björn and Seppi, Dino and Steidl, Stefan and Devillers, Laurence and Vidrascu, Laurence and Vogt, Thurid and Aharonson, Vered and Amir, Noam},
	date = {2011-01-01},
	doi = {10.1007/978-3-642-15184-2_6},
	note = {Journal Abbreviation: Cognitive Technologies},
	file = {Full Text PDF:C\:\\Users\\salam\\Zotero\\storage\\GCBZRSBI\\Batliner et al. - 2011 - The Automatic Recognition of Emotions in Speech.pdf:application/pdf}
}

@inproceedings{schwenker_gmm-svm_2009,
	location = {Berlin, Heidelberg},
	title = {The {GMM}-{SVM} Supervector Approach for the Recognition of the Emotional Status from Speech},
	isbn = {978-3-642-04274-4},
	doi = {10.1007/978-3-642-04274-4_92},
	series = {Lecture Notes in Computer Science},
	abstract = {Emotion recognition from speech is an important field of research in human-machine-interfaces, and has various applications, for instance for call centers. In the proposed classifier system {RASTA}-{PLP} features (perceptual linear prediction) are extracted from the speech signals. The first step is to compute an universal background model ({UBM}) representing a general structure of the underlying feature space of speech signals. This {UBM} is modeled as a Gaussian mixture model ({GMM}). After computing the {UBM} the sequence of feature vectors extracted from the utterance is used to re-train the {UBM}. From this {GMM} the mean vectors are extracted and concatenated to the so-called {GMM} supervectors which are then applied to a support vector machine classifier. The overall system has been evaluated by using utterances from the public Berlin emotional database. Utilizing the proposed features a recognition rate of 79\% (utterance based) has been achieved which is close to the performance of humans on this database.},
	pages = {894--903},
	booktitle = {Artificial Neural Networks – {ICANN} 2009},
	publisher = {Springer},
	author = {Schwenker, Friedhelm and Scherer, Stefan and Magdi, Yasmine M. and Palm, Günther},
	editor = {Alippi, Cesare and Polycarpou, Marios and Panayiotou, Christos and Ellinas, Georgios},
	date = {2009},
	langid = {english},
	keywords = {Automatic Speech Recognition, Emotion Recognition, Gaussian Mixture Model, Speech Signal, Support Vector Machine},
	file = {Springer Full Text PDF:C\:\\Users\\salam\\Zotero\\storage\\LSMZ4EFR\\Schwenker et al. - 2009 - The GMM-SVM Supervector Approach for the Recogniti.pdf:application/pdf}
}

@article{heracleous_comprehensive_2019,
	title = {A comprehensive study on bilingual and multilingual speech emotion recognition using a two-pass classification scheme},
	volume = {14},
	issn = {1932-6203},
	url = {http://dx.plos.org/10.1371/journal.pone.0220386},
	doi = {10.1371/journal.pone.0220386},
	pages = {e0220386},
	number = {8},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Heracleous, Panikos and Yoneyama, Akio},
	editor = {Shahamiri, Seyed Reza},
	urldate = {2020-04-09},
	date = {2019-08-15},
	langid = {english},
	file = {Heracleous and Yoneyama - 2019 - A comprehensive study on bilingual and multilingua.pdf:C\:\\Users\\salam\\Zotero\\storage\\TZPUZ99D\\Heracleous and Yoneyama - 2019 - A comprehensive study on bilingual and multilingua.pdf:application/pdf}
}

@inproceedings{xia_dbn-ivector_2016,
	title = {{DBN}-ivector Framework for Acoustic Emotion Recognition},
	url = {http://www.isca-speech.org/archive/Interspeech_2016/abstracts/0488.html},
	doi = {10.21437/Interspeech.2016-488},
	abstract = {Deep learning and i-vectors have been successfully used in speech and speaker recognition recently. In this work we propose a framework based on deep belief network ({DBN}) and ivector space modeling for acoustic emotion recognition. We use two types of labels for frame level {DBN} training. The ﬁrst one is the vector of posterior probabilities calculated from the {GMM} universal background model ({UBM}). The second one is the predicted label based on the {GMMs}. The {DBN} is trained to minimize errors for both types. After {DBN} training, we use the vector of posterior probabilities estimated by {DBN} to replace the {UBM} for i-vector extraction. Finally the extracted i-vectors are used in backend classiﬁers for emotion recognition. Our experiments on the {USC} {IEMOCAP} data show the effectiveness of our proposed {DBN}-ivector framework. In particular, with decision level combination, our proposed system yields signiﬁcant improvement on both unweighted and weighted accuracy.},
	eventtitle = {Interspeech 2016},
	pages = {480--484},
	author = {Xia, Rui and Liu, Yang},
	urldate = {2020-04-09},
	date = {2016-09-08},
	langid = {english},
	file = {Xia and Liu - 2016 - DBN-ivector Framework for Acoustic Emotion Recogni.pdf:C\:\\Users\\salam\\Zotero\\storage\\M9GBC7DI\\Xia and Liu - 2016 - DBN-ivector Framework for Acoustic Emotion Recogni.pdf:application/pdf}
}

@software{sadjadi_msr_2013,
	title = {{MSR} Identity Toolbox},
	author = {Sadjadi, Seyed Omid and Slaney, Malcolm and Heck, Larry},
	date = {2013-10-17},
	file = {Sadjadi - MSR Identity Toolbox.pdf:C\:\\Users\\salam\\Zotero\\storage\\QVDHX93S\\Sadjadi - MSR Identity Toolbox.pdf:application/pdf}
}